{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382de452-d0b0-40ee-b267-353c9c96ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1dd1e10-c701-465e-86d0-7ff157203cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "for w in stopwords.words('english'):\n",
    "    print(w)\n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([w for w in text.split() if w not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9d9924-22e7-4e0d-bb37-778f0de82b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps lazy dog. This simple example text document retrieval.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('doc1.txt', encoding='utf8')\n",
    "doc1 = file.read()\n",
    "doc1\n",
    "\n",
    "doc1=remove_stopwords(doc1)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46542ce8-4c14-48d8-929c-dd52f3c118a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29793a98-b6f1-4688-a45d-9700e8aee390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A lazy dog lies tree fox runs away. Document retrieval helps finding relevant information.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('doc2.txt', encoding='utf8')\n",
    "doc2 = file.read()\n",
    "doc2\n",
    "\n",
    "doc2=remove_stopwords(doc2)\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b07b6e2-38df-47cf-bf7e-a9c7ac22caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = remove_punctuation(doc1.lower()).split()\n",
    "tokens2 = remove_punctuation(doc2.lower()).split()\n",
    "\n",
    "terms = list(set(tokens1 + tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f56cf9-9f4d-418b-8b23-31b6836fe3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = {}\n",
    "\n",
    "for term in terms:\n",
    "    documents = []\n",
    "    if term in tokens1:\n",
    "        documents.append(\"Document 1\")\n",
    "        inv_index[term] = documents\n",
    "    if term in tokens2:\n",
    "        documents.append(\"Document 2\")\n",
    "        inv_index[term] = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9037de-c049-4880-84fe-bdd79b68edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the \t -> Document 1\n",
      "runs \t -> Document 2\n",
      "retrieval \t -> Document 1, Document 2\n",
      "lazy \t -> Document 1, Document 2\n",
      "example \t -> Document 1\n",
      "document \t -> Document 1, Document 2\n",
      "jumps \t -> Document 1\n",
      "helps \t -> Document 2\n",
      "dog \t -> Document 1, Document 2\n",
      "simple \t -> Document 1\n",
      "finding \t -> Document 2\n",
      "fox \t -> Document 1, Document 2\n",
      "quick \t -> Document 1\n",
      "a \t -> Document 2\n",
      "tree \t -> Document 2\n",
      "relevant \t -> Document 2\n",
      "information \t -> Document 2\n",
      "text \t -> Document 1\n",
      "lies \t -> Document 2\n",
      "away \t -> Document 2\n",
      "brown \t -> Document 1\n",
      "this \t -> Document 1\n"
     ]
    }
   ],
   "source": [
    "for term, documents in inv_index.items():\n",
    "    print(term,\"\\t\", \"->\",\", \".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad07917-18e1-4e95-b316-98f02682ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d5676b-a18d-4e9b-a9d4-3e45d1d5c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('' , '' , string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11289557-6d2d-4152-99b9-e14e3078cbaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "for w in stopwords.words('english'):\n",
    "    print(w)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in text.split() if w not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31155716-ab67-46fc-b72f-59b741c57a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. This is a simple example text for document retrieval.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('doc1.txt' ,encoding='utf-8')\n",
    "doc1 = file.read()\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba0220-ad37-4c6c-8963-aee28719101f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d17d97-afcd-4f00-b71b-2ddba6fde135",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = remove_stopwords(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339ea444-4052-4aff-844c-c947ab916d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps lazy dog. This simple example text document retrieval.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "622c3dd9-e65e-4dfe-b19f-4d53c447a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('doc2.txt' , encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f64c0e0-28d9-4226-9cfa-31c7237fcf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c0c94ac-4a40-471a-bb18-d54f83fe0a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A lazy dog lies under the tree while the fox runs away. Document retrieval helps in finding relevant information.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7986a083-1f97-404e-bf49-450c27d4b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = remove_stopwords(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "377bfbd2-e6e8-4bd3-b318-fea4b63ba36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A lazy dog lies tree fox runs away. Document retrieval helps finding relevant information.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e532913-674f-458d-a6d2-dc771e75b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = remove_punctuation(doc1.lower()).split()\n",
    "tokens2 = remove_punctuation(doc2.lower()).split()\n",
    "\n",
    "terms = list(set(tokens1+tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17e199ae-66db-4136-8176-2a15748c51d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'runs',\n",
       " 'retrieval',\n",
       " 'lazy',\n",
       " 'example',\n",
       " 'document',\n",
       " 'jumps',\n",
       " 'helps',\n",
       " 'dog',\n",
       " 'simple',\n",
       " 'finding',\n",
       " 'fox',\n",
       " 'quick',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'relevant',\n",
       " 'information',\n",
       " 'text',\n",
       " 'lies',\n",
       " 'away',\n",
       " 'brown',\n",
       " 'this']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06caec5c-659f-4aea-b45f-e0936c9980d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = {}\n",
    "\n",
    "for term in terms:\n",
    "    documents=[]\n",
    "    if term in tokens1:\n",
    "        documents.append('Document1')\n",
    "        inv_index[term] = documents\n",
    "    if term in tokens2:\n",
    "        documents.append('Document2')\n",
    "        inv_index[term] = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a222b494-667e-4510-8d02-25dbfcf805f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['Document1'],\n",
       " 'runs': ['Document2'],\n",
       " 'retrieval': ['Document1', 'Document2'],\n",
       " 'lazy': ['Document1', 'Document2'],\n",
       " 'example': ['Document1'],\n",
       " 'document': ['Document1', 'Document2'],\n",
       " 'jumps': ['Document1'],\n",
       " 'helps': ['Document2'],\n",
       " 'dog': ['Document1', 'Document2'],\n",
       " 'simple': ['Document1'],\n",
       " 'finding': ['Document2'],\n",
       " 'fox': ['Document1', 'Document2'],\n",
       " 'quick': ['Document1'],\n",
       " 'a': ['Document2'],\n",
       " 'tree': ['Document2'],\n",
       " 'relevant': ['Document2'],\n",
       " 'information': ['Document2'],\n",
       " 'text': ['Document1'],\n",
       " 'lies': ['Document2'],\n",
       " 'away': ['Document2'],\n",
       " 'brown': ['Document1'],\n",
       " 'this': ['Document1']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49e896f8-3273-484f-92cc-7c662b4023cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the \t -> Document1\n",
      "runs \t -> Document2\n",
      "retrieval \t -> Document1, Document2\n",
      "lazy \t -> Document1, Document2\n",
      "example \t -> Document1\n",
      "document \t -> Document1, Document2\n",
      "jumps \t -> Document1\n",
      "helps \t -> Document2\n",
      "dog \t -> Document1, Document2\n",
      "simple \t -> Document1\n",
      "finding \t -> Document2\n",
      "fox \t -> Document1, Document2\n",
      "quick \t -> Document1\n",
      "a \t -> Document2\n",
      "tree \t -> Document2\n",
      "relevant \t -> Document2\n",
      "information \t -> Document2\n",
      "text \t -> Document1\n",
      "lies \t -> Document2\n",
      "away \t -> Document2\n",
      "brown \t -> Document1\n",
      "this \t -> Document1\n"
     ]
    }
   ],
   "source": [
    "for term , document in inv_index.items():\n",
    "    print(term , \"\\t\",\"->\", \", \".join(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c651d52a-8c1f-4785-974d-edccfb494b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your word document\n"
     ]
    }
   ],
   "source": [
    "xx = input('Enter your word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adfcc9bd-5c0a-4422-a254-8cfd23dce85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document -> document1,document2\n"
     ]
    }
   ],
   "source": [
    "if xx in tokens1 and xx not in tokens2:\n",
    "    print(xx , '----> document1')\n",
    "\n",
    "elif xx in tokens1 and tokens2:\n",
    "    print(xx, '-> document1,document2')\n",
    "\n",
    "elif xx in tokens2 and xx not in tokens1:\n",
    "    print(xx , '-----> document2')\n",
    "\n",
    "else:\n",
    "    print('the entered word is not in documents')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70812478-2778-4f7e-a8c7-90459ec6a19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9030ec6-1ddf-4f0f-be71-006b0b9a97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e84f67-7c8f-4c1d-89ba-4010dbe47bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index: ---- Posting lists\n",
      "document:---> ['doc1', 'doc2', 'doc3']\n",
      "sample:---> ['doc1']\n",
      "a:---> ['doc1']\n",
      "machine:---> ['doc1', 'doc3']\n",
      "this:---> ['doc1', 'doc2']\n",
      "learning:---> ['doc1', 'doc2', 'doc3']\n",
      "is:---> ['doc1']\n",
      "about:---> ['doc1', 'doc3']\n",
      "discusses:---> ['doc2']\n",
      "of:---> ['doc2']\n",
      "the:---> ['doc2']\n",
      "deep:---> ['doc2']\n",
      "applications:---> ['doc2']\n",
      "intelligence:---> ['doc3']\n",
      "another:---> ['doc3']\n",
      "and:---> ['doc3']\n",
      "talking:---> ['doc3']\n",
      "artificial:---> ['doc3']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample documents as a dictionary\n",
    "documents = {\n",
    "    \"doc1\": \"This is a sample document about machine learning.\",\n",
    "    \"doc2\": \"This document discusses the applications of deep learning.\",\n",
    "    \"doc3\": \"Another document talking about artificial intelligence and machine learning.\"\n",
    "}\n",
    "\n",
    "# Preprocessing function to clean text and remove special characters\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()  # Tokenize text\n",
    "    return words\n",
    "\n",
    "# Create inverted index\n",
    "def create_inverted_index(docs):\n",
    "    inverted_index = defaultdict(list)  # Dictionary to store the inverted index\n",
    "    for doc_name, content in docs.items():\n",
    "        words = preprocess_text(content)  # Process each document's text\n",
    "        for word in set(words):  # Use set to avoid duplicate entries for the same word in a document\n",
    "            inverted_index[word].append(doc_name)  # Map word to the document it appears in\n",
    "    return dict(inverted_index)\n",
    "\n",
    "# Build the inverted index\n",
    "inverted_index = create_inverted_index(documents)\n",
    "\n",
    "# Display the inverted index\n",
    "print(\"Inverted Index: ---- Posting lists\")\n",
    "for word, doc_list in inverted_index.items():\n",
    "    print(f\"{word}:---> {doc_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09b19e-e861-4e6c-95a7-40a8a7761d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
